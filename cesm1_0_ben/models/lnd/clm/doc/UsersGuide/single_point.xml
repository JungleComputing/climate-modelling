
<!-- Beg of single_point chapter-->
<chapter id="single_point">
<title>How to run Single-Point/Regional cases</title>
<para>
The &clm; also allows you to set up and run cases with a single-point or a local region as well
as global resolutions. This is often useful for running quick cases for testing, evaluating
specific vegetation types, or land-units, or running with observed data for a specific site.
There are three different ways to do this: <envar>PTS_MODE</envar>,
&CLM1PT;, and &CLMUSRDAT;.
<simplelist>
<member><emphasis><envar>PTS_MODE</envar></emphasis> -- to run for a single point
using global datasets.</member>
<member><emphasis>&CLM1PT;</emphasis> -- to run for a supported single-point
or regional dataset.</member>
<member><emphasis>&CLMUSRDAT;</emphasis> -- to run using your own datasets (single-point
or regional).</member>
</simplelist>
</para>
<note>
<para>
<envar>PTS_MODE</envar> only works for a single point, while the other two options can
also work for regional datasets as well.
</para>
</note>

<sect1 id="PTS_MODE">
<title>Running PTS_MODE configurations</title>
<para>
<envar>PTS_MODE</envar> enables you to run the model using global datasets, but just picking a
single point from those datasets and operating on it. It can be a very quick way to do fast
simulations and get a quick turnaround.
</para>
<para>
To setup a <envar>PTS_MODE</envar> simulation you use the "-pts_lat" and "-pts_lon"
arguments to create_newcase to give the latitude and longitude of the point you want to
simulate for (the code will pick the point on the global grid nearest to the point you
give. Here's an example to setup a simulation for the nearest point at 2-degree resolution
to Boulder Colorado.
<screen width="99">
> cd scripts
> create_newcase -case testPTS_MODE -res f19_g16 -compset I -mach bluefire \
-pts_lat 40.0 -pts_lon -105
> cd testPTS_MODE
# We make sure the model will start up cold rather than using initial conditions
> xmlchange -file env_conf.xml -id &CLMFORCECOLD; -val on
</screen>
Then configure, build and run as normal. We make sure initial conditions are NOT used
since <envar>PTS_MODE</envar> currently CAN NOT run with initial conditions.
</para>
<important>
<para>
By default it sets up to run with
<envar>USE_MPISERIAL</envar> (in the <filename>env_builld.xml</filename> file) turned on, 
which allows you to run the model interactively. On some machines this mode is NOT 
supported and you may need to change it to FALSE before you are able to build.
</para>
</important>
<warning>
<para>
<envar>PTS_MODE</envar> currently does <emphasis>NOT</emphasis> restart nor
is it able to startup from global initial condition files.
</para>
</warning>
<note>
<para>
You can change the point you are simulating for at run-time by changing the values of
<envar>PTS_LAT</envar> and <envar>PTS_LON</envar> in the <filename>env_run.xml</filename> file.
</para>
</note>
<note>
<para>
Note, that when running with <envar>PTS_MODE</envar> the number of processors
is automatically set to one. When running a single grid point you can only use a single
processor. You might also want to set the "env_conf" variable: <envar>USE_MPISERIAL</envar> to
<literal>TRUE</literal> so that you can also run interactively without having to use
mpi to start up your job.
</para>
</note>
</sect1>

<sect1 id="suprted_single_point_datasets">
<title>Running Supported Single-point/Regional Datasets</title>
<para>
In addition to <envar>PTS_MODE</envar> the &clm; supports running using single-point or
regional datasets that are customized to a particular region. In the section below we
tell the user how to create their own dataset, but we also support a small number of
single-point and regional datasets that are ready to setup and run in the CCSM modeling
system.
</para>
<para>
To get the list of supported dataset resolutions see 
<link endterm="CLM1PT" linkend="CLM1PT"></link>, which results in the following:
<screen width="99">
&res_list;
</screen>
The resolution names that have an underscore in them ("_") are all single-point or 
regional resolutions.
To run with the supported single-point and regional datasets, you setup a simulation for the
"pt1_pt1" resolution and give the short-name for the file to use in the
<filename>env_conf.xml</filename> file. Then to run for the urban Mexico City Mexico test site
do the following:
<screen width="99">
> cd scripts
> create_newcase -case testSPDATASET -res pt1_pt1 -compset I \
-mach bluefire
> cd testSPDATASET
> xmlchange -file env_conf.xml -id &CLM1PT; -val 1x1_mexicocityMEX
</screen>
Then configure, build and run normally.
</para>
<important>
<para>
Just like <envar>PTS_MODE</envar> above, By default it sets up to run with
<envar>USE_MPISERIAL</envar> (in the <filename>env_build.xml</filename> file) turned on, 
which allows you to run the model interactively. On some machines this mode is NOT 
supported and you may need to change it to FALSE before you are able to build.
</para>
</important>
<note>
<para>
Note, that when running a <literal>pt1_pt1</literal> resolution the number of processors
is automatically set to one. When running a single grid point you can only use a single
processor. You might also want to set the "env_conf" variable: <envar>USE_MPISERIAL</envar> to
<literal>TRUE</literal> so that you can also run interactively without having to use
mpi to start up your job.
</para>
</note>
<sect2 id="sp_atm_forcing">
<title>Running Supported Single-point Datasets that have their own Atmospheric Forcing</title>
<para>
Of the supported single-point datasets we have three that also have atmospheric forcing data 
that go with them: Mexico City (Mexico), Vancouver, (Canada, British Columbia), and
urbanc_alpha (test data for an Urban intercomparison project). Mexico city and Vancouver
also have "#ifdef" in the source code for them to work with modified urban data
parameters that are particular to these locations. They can be turned on by using
the &CLMCONFIG; &envconf; variable to set the "-cppdefs" option in the &clm;
&configure;. To turn on the atmospheric forcing for these datasets, you set the
&envconf; <envar>DATM_MODE</envar> variable to "CLM1PT", and then the atmospheric
forcing datasets will be used for the point picked. In the example below we will
show how to do this for the Vancouver, Canada point.
<example id="vancouver">
<title>Example of running &clm; over the single-point of Vancouver Canada with 
supplied atmospheric forcing data for Vancouver
</title>
<screen width="99">
> cd script
# Create a case at the single-point resolutions
> create_newcase -case testSPDATASETnAtmForcing -res pt1_pt1 -compset I \
-mach bluefire
> cd testSPDATASETnAtmForcing
# NOTE: For Mexico City and Vancouver there are special ifdef's for the urban model 
# that can be turned on. For a generic, case you would NOT do the following step.
> xmlchange -file env_conf.xml -id &CLMCONFIG; -val "-cppdefs VANCOUVER"
# Now set the CLM single-point variable to the Vancouver Canada name
> xmlchange -file env_conf.xml -id &CLM1PT; -val 1x1_vancouverCAN
# Set the aerosols to use the single-point dataset for Vancouver for 2000 conditions
# You could also use the default global dataset, but running would be a bit slower
> xmlchange -file env_conf.xml -id DATM_MODE -val CLM1PT
</screen>
</example>
</para>
</sect2>
</sect1>

<sect1 id="own_single_point_datasets">
<title>Creating your own single-point/regional surface datasets</title>
<para>
The file:
<ulink url="../Quickstart.userdatasets">Quickstart.userdatasets</ulink> in the
<filename>models/lnd/clm/doc</filename> directory gives guidelines on how to create and run
with your own single-point or regional datasets. Below we reprint the above guide.
<programlisting width="99">
&quickstart_userdata;
</programlisting>
</para>

<sect2 id="getregional_datasets.pl">
<title>Using getregional_datasets.pl to get a complete suite of single-point/regional 
surface datasets from global ones</title>
<para>
Use the regional extraction script to get regional datasets from the global ones
The getregional_datasets.pl script to extract out regional datasets of interest.
Note, the script works on all files other than the "finidat" file as it's a 1D vector file.
Also, note that it works on the deposition files without streams, so
<envar>DATM_PRESAERO</envar> MUST be "none", and you must set "-ndepsrc" to "data" in
build-namelist.
The script will extract out a block of gridpoints from all the input global datasets,
and create the full suite of input datasets to run over that block. The input datasets
will be named according to the input "id" you give them and the id can then be used
as input to &CLMUSRDAT; to create a case that uses it. See
the section on <link linkend="clm_script">&clm; Script Configuration Items</link> for 
more information on setting &CLMUSRDAT; (in <xref 
linkend="customize"></xref>). The list of files extracted by
their name used in the namelists are:
<varname>fatmgrid</varname>, <varname>fatmlndfrc</varname>, 
<varname>fsurdat</varname>, <varname>fpftdyn</varname>, 
<varname>flndtopo</varname>, <varname>faerdep</varname>, <varname>fndepdat</varname>,
<varname>fndepdyn</varname>, and the datm file <varname>domainfile</varname>.
For more information on these files see the <link linkend="required_files"
>Table on required files</link>.
</para>
<para>
The alternatives to using this script are to use <envar>PTS_MODE</envar>,
discussed earlier, or creating the files individually using the different 
file creation tools (given in the <link linkend="tools">Tools Chapter</link>). Creating
all the files individually takes quite a bit of effort and time. <envar>PTS_MODE</envar>
has some limitations as discussed earlier, but also as it uses global files, is
a bit slower when running simulations than using files that just have the set
of points you want to run over. Another advantage is that once you've created the
files using this script you can customize them if you have data on this specific
location that you can replace with what's already in these files.
</para>
<para>
The script requires the use of both "Perl" and "NCL". See the <link
linkend="ncl_scripts">NCL Script</link> section in the <link linkend="tools">Tools Chapter</link>
on getting and using NCL and NCL scripts. The main script to use is a perl script 
which will then in turn call the NCL script that actually creates the output files. 
The ncl script gets it's settings from environment variables set by the perl script.
To get help with the script use "-help" as follows:
<screen width="99">
> cd models/lnd/clm/tools/ncl_scripts
> getregional_datasets.pl -help
</screen>
The output of the above is:
<screen width="99">
&getreg_datasets;
</screen>
</para>
<para>
The <emphasis>required</emphasis> options are: <varname>-id</varname>,
<varname>-ne</varname>, and <varname>-se</varname>, for the output identifier
name to use in the filenames, latitude and longitude of the Northeast corner, and
latitude and longitude of the SouthEast corner (in degrees). Options that specify
which files will be used are: <varname>-mask</varname>, <varname>-res</varname>,
<varname>-rcp</varname>, <varname>-sim_year</varname>, and <varname>-sim_yr_rng</varname>
for the land-mask to use, global resolution name, representative concentration pathway
for future scenarios, simulation year, and simulation year range. The location of the 
input and output files will be determined by the option <varname>-mycsmdata</varname> 
(can also be set by using the environment variable <varname>$CSMDATA</varname>). If
you are running on a machine like at NCAR where you do NOT have write permission
to the CCSM inputdata files, you should use the <filename>scripts/link_dirtree</filename>
script to create softlinks of the original files to a location that you can write
to. This way you can use both your new files you created as well as the original
files and use them from the same location.
</para>
<para>
The remaining options to the script are <varname>-debug</varname>,
and <varname>-verbose</varname>. <varname>-debug</varname> is used to show what
would happen if the script was run, without creating the actual files.
<varname>-verbose</varname> adds extra log output while creating the files so you
can more easily see what the script is doing.
</para>
<para>
For example, Run the extraction for data from 52-73 North latitude, 190-220 longitude
that creates 13x12 gridcell region from the f19 (1.9x2.5) global resolution over Alaska.
<example id="example_getregional_datasets">
<title>Example of running <command>getregional_datasets.pl</command> to get
datasets for a specific region over Alaska</title>
<screen width="99">
> cd scripts
# First make sure you have a inputdata location that you can write to 
# You only need to do this step once, so you won't need to do this in the future
> setenv MYCSMDATA "directory to put your inputdata files"
> link_dirtree $CSMDATA $MYCSMDATA
> cd ../models/lnd/clm/tools/ncl_scripts
> getregional_datasets.pl -sw 52,190 -ne 73,220 -id 13x12pt_f19_alaskaUSA -mycsmdata $MYCSMDATA
</screen>
</example>
Repeat this process if you need files for multiple sim_year, resolutions, land-masks, 
and sim_year_range values.
</para>
<para>
Now to run a simulation with the datasets created above, you create a single-point
case, and set &CLMUSRDAT; to the identifier used above. Note that in the example below
we set the number of processors to use to one (-pecount 1). For a single point, you
should only use a single processor, but for a regional grid, such as the example below
you could use up to the number of grid points (12x13=156 processors.
</para>
<example id="example_using_clmusrdat">
<title>Example of using &CLMUSRDAT; to run a simulation using user datasets for a
specific region over Alaska</title>
<screen width="99">
> cd scripts
# Create the case and set it to only use one processor
> create_newcase -case my_userdataset_test -res f19_g16 -compset I1850 \
-mach bluefire -pecount 1
> cd my_userdataset_test/
> xmlchange -file env_run.xml -id DIN_LOC_ROOT_CSMDATA -val $MYCSMDATA
> xmlchange -file env_conf.xml -id &CLMUSRDAT; -val 13x12pt_f19_alaskaUSA
> configure -case
</screen>
</example>
</sect2>

</sect1>

<sect1 id="own_atm_forcing">
<title>Running with your own atmosphere forcing</title>
<para>
Here we want to run with our own customized datasets for &clm; as well as 
running with our own supplied atmosphere forcing datasets. Thus we effectively
combine the information from <xref linkend="sp_atm_forcing"></xref> with 
<xref linkend="own_single_point_datasets"></xref>. First we need to follow
the procedures in <xref linkend="sp_atm_forcing"></xref> to come up with &clm;
datasets that are customized for our point or region in question. This includes
running <command>link_dirtree</command> to create a directory location where you
can add your own files to it. Next, set
<envar>DATM_MODE</envar> to "CLM1PT" and &CLM1PT; and &CLMUSRDAT; to the
id of the data you created. To see a list of what the filenames need to be
see the section on setting <link linkend="CLMUSRDAT">&CLMUSRDAT;</link>.
</para>
<para>
Next we need to setup the atmosphere forcing data in &netcdf; format that can be
read by datm. There is a list of ten variables that are expected to be on the input
files with the names and units on the following table (in the table TDEW and SHUM
are optional fields that can be used in place of RH). In the table we also list
which of the fields are required and if not required what the code will do to
replace them. If the names of the fields are different or the list is changed
from the standard list of the ten fields: FLDS, FSDS, FSDSdif, FSDSdir, PRECTmms, 
PSRF, RH, TBOT, WIND, and ZBOT, the resulting streams file will need to be modified
to take this into account.
<table id="atm_forcing_fields" tocentry="1" pgwide="1" frame="all">
<title>Atmosphere Forcing Fields</title>
<tgroup cols="4" align="left" colsep="1" rowsep="1">
<thead>
<row>
   <entry><para>Short-name</para></entry>
   <entry><para>Description</para></entry>
   <entry><para>Units</para></entry>
   <entry><para>Required?</para></entry>
   <entry><para>If NOT required how replaced</para></entry>
</row>
</thead>
<tbody>
<row>
   <entry>FLDS</entry><entry>incident longwave
(FLDS)</entry><entry>W/m2</entry><entry>No</entry>
<entry>calculates based on Temperature, Pressure and Humidity</entry>
</row>
<row>
   <entry>FSDS</entry><entry>incident solar
(FSDS)</entry><entry>W/m2</entry><entry>Yes</entry><entry>-</entry>
</row>
<row>
   <entry>FSDSdif</entry><entry>incident solar (FSDS)
diffuse</entry><entry>W/m2</entry><entry>No</entry><entry>based on FSDS</entry>
</row>
<row>
   <entry>FSDSdir</entry><entry>incident solar (FSDS)
direct</entry><entry>W/m2</entry><entry>No</entry><entry>based on FSDS</entry>
</row>
<row>
   <entry>PRECTmms</entry><entry>precipitation
(PRECTmms)</entry><entry>mm/s</entry><entry>Yes</entry><entry>-</entry>
</row>
<row>
   <entry>PSRF</entry><entry>pressure at the lowest atm level
(PSRF)</entry><entry>Pa</entry><entry>No</entry><entry>assumes standard-pressure</entry>
</row>
<row>
   <entry>RH</entry><entry>relative humidity at the lowest atm level
(RH)</entry><entry>%</entry><entry>No</entry><entry>can be replaced with SHUM or TDEW</entry>
</row>
<row>
   <entry>SHUM</entry><entry>specific humidity at the lowest atm level
</entry><entry>kg/kg</entry><entry>Optional in place of RH</entry><entry>can be replaced with RH or TDEW</entry>
</row>
<row>
   <entry>TBOT</entry><entry>temperature at the lowest atm level
(TBOT)</entry><entry>K (or can be C)</entry><entry>Yes</entry><entry>-</entry>
</row>
<row>
   <entry>TDEW</entry><entry>dew point temperature
</entry><entry>K (or can be C)</entry><entry>Optional in place of RH</entry><entry>can be replaced with RH or SHUM</entry>
</row>
<row>
   <entry>WIND</entry><entry>wind at the lowest atm level
(WIND)</entry><entry>m/s</entry><entry>Yes</entry><entry>-</entry>
</row>
<row>
   <entry>ZBOT</entry><entry>observational height</entry><entry>m</entry><entry>No
</entry><entry>assumes 30 meters</entry>
</row>
</tbody>
</tgroup>
</table>
All of the variables should be dimensioned: time, lat, lon, with time being the unlimited
dimension. The coordinate variable "time" is also required with CF-compliant units in
days, hours, minutes, or seconds. It can also have a calendar attribute that can
be "noleap" or "gregorian". Normally the files will be placed in the:
<filename>$CSMDATA/atm/datm7/domain.clm</filename> directory with separate files per
month called <filename>clm1pt-YYYY-MM.nc</filename> where YYYY-MM corresponds to the four
digit year and two digit month with a dash inbetween.
<screen width="99">
> cd scripts
# First make sure you have a inputdata location that you can write to 
# You only need to do this step once, so you won't need to do this in the future
> setenv MYCSMDATA "directory to put your inputdata files"
> link_dirtree $CSMDATA $MYCSMDATA
# Next create and move all your datasets into $MYCSMDATA with id $MYUSRDAT
# See above for naming conventions

#  Now create a single-point case
> create_newcase -case my_atmforc_test -res pt1_pt1 -compset I1850 \
-mach bluefire
> cd my_atmforc_test
# Set the data root to your inputdata directory, and set &CLM1PT; and &CLMUSRDAT; 
# to the user id you created for your datasets above
> xmlchange -file env_run.xml -id DIN_LOC_ROOT_CSMDATA -val $MYCSMDATA
> xmlchange -file env_conf.xml -id &CLM1PT; -val $MYUSRDAT
> xmlchange -file env_conf.xml -id &CLMUSRDAT; -val $MYUSRDAT
# Then set DATM_MODE to single-point mode so datm will use your forcing datasets
# Put your forcing datasets into $MYCSMDATA/atm/datm7/domain.clm
> xmlchange -file env_conf.xml -id DATM_MODE -val CLM1PT
> configure -case
# If the list of fields, or filenames, filepaths, or fieldnames are different 
# you'll need to edit the datm namelist streams file to make it consistent
> $EDITOR Buildconf/datm.buildnml.csh
</screen>
</para>
</sect1>

</chapter>
<!-- End of single_point chapter -->
